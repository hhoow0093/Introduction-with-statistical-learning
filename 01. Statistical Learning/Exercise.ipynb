{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc32GZL2xcMO+VBwwU69+2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Learning Exercise"
      ],
      "metadata": {
        "id": "y_KrkKtVLAQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flexible Approach vs Inflexible Approach"
      ],
      "metadata": {
        "id": "rijXUbGkbb45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 . For each of parts (a) through (d), indicate whether we would generally\n",
        "expect the performance of a flexible statistical learning method to be\n",
        "better or worse than an inflexible method. Justify your answer.\n",
        "\n",
        "(a) The sample size n is extremely large, and the number of predictors p is small.\n",
        "\n",
        "(b) The number of predictors p is extremely large, and the number\n",
        "of observations n is small.\n",
        "\n",
        "(c) The relationship between the predictors and response is highly\n",
        "non-linear.\n",
        "\n",
        "(d) The variance of the error terms, i.e. σ\n",
        "2 = Var(ϵ), is extremely\n",
        "high.\n",
        "\n",
        "> **inflexible method** : inflexible method compromised algorithms such as *simple Decision Tree, linear regression, logistic regression, Lasso regression, linear discriminant analysis, naive bayes* because the model can be interpreted by manual mathematics calculation\n",
        "\n",
        "> **flexible method** : flexible method is very hard to compute it manually by mathematics because the model is already complex. Hence could cause over fitting in a model. Example algorithm are *Support vector model, K-nearest neighbour, complex decision trees, deep random forest, neural network*\n"
      ],
      "metadata": {
        "id": "X7S96ogcLFtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ],
      "metadata": {
        "id": "5u6m3cqwjWEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) . If a there is too much sample and less predictor, the likelyhood of underfitting of a model could happen when we use **inflexible algorithm** can happen easily because model could potentially not generalize the relationships for each records therefore *creating a model learns patterns poorly for each data.* Therefore *to learn better patterns for each data relationship*, performing **flexible method** is better because it wont cause underfitting and the model can generalize the pattern better for model evaluation."
      ],
      "metadata": {
        "id": "_Wih36wfLY-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) . When the number of predictors is large and the number of observations is small, using a flexible method can easily lead to overfitting.\n",
        "Because there are too few data points relative to the number of predictors, a flexible method can fit the training data almost perfectly — essentially memorizing the noise instead of learning the true relationship.\n",
        "As a result, it performs poorly on unseen data.\n",
        "Therefore, in this situation, an inflexible method is preferable, since it imposes more structure on the model and is less likely to overfit, allowing for better generalization despite limited data."
      ],
      "metadata": {
        "id": "84V1OgMlRvyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) . relationship between predictors and dependent variable is highly non linear. Therefore, using **flexible method** is better because it helps the model to learn the pattern for all the existing variables between dependent and independent to generalize a better pattern for a model."
      ],
      "metadata": {
        "id": "SJqC928hU8Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) . if variance is very high, it means that the data distribution is not close with the mean or average. Therefore the spread of data is quite large and we need to detect what is causing this high variance. It could be a stupid **high amount of outlier** or some **complex relationships** between independent variables and dependennt variables. if outlier, performing inflexible method is better because we can throw out the outlier and generalize the model structure better with the dependent variable. If the variance is caused by the complex relationship its self, then using flexible method is better because for each different sample we use, we can perform why this sample group behaves this way and this also applies with the other sample groups."
      ],
      "metadata": {
        "id": "6SMcZknuXyug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification vs Regression (predict or inference)"
      ],
      "metadata": {
        "id": "lvMS9MihbiXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 . Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\n",
        "\n",
        "(a) We collect a set of data on the top 500 firms in the US. For each\n",
        "firm we record profit, number of employees, industry and the\n",
        "CEO salary. We are interested in understanding which factors\n",
        "affect CEO salary.\n",
        "\n",
        "(b) We are considering launching a new product and wish to know\n",
        "whether it will be a success or a failure. We collect data on 20\n",
        "similar products that were previously launched. For each product we have recorded whether it was a success or failure, price\n",
        "charged for the product, marketing budget, competition price,\n",
        "and ten other variables.\n",
        "\n",
        "(c) We are interested in predicting the % change in the USD/Euro\n",
        "exchange rate in relation to the weekly changes in the world\n",
        "stock markets. Hence we collect weekly data for all of 2012. For\n",
        "each week we record the % change in the USD/Euro, the %\n",
        "change in the US market, the % change in the British market,\n",
        "and the % change in the German market."
      ],
      "metadata": {
        "id": "-lOh1EbGbPNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ],
      "metadata": {
        "id": "jQQ-IUPRjcgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) . If we are interested which factors affect the CEO salary. Then, this is an **inference problem**. we want to estimate *f* and figure out which **predictor** has the most significant impact with the **dependent variable**. To estimate *f* for this problem, **regression** is used because the dependent variable (salary) is a *continuous value* and that value is affected by the predictors."
      ],
      "metadata": {
        "id": "J_LQL3K8dm8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b). If we are interested in predicting whether similar products that were launched success or not to consider launching new product. Then this should be a classification problem.  "
      ],
      "metadata": {
        "id": "oqSxHRuYhzJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference problem cheatcheet\n"
      ],
      "metadata": {
        "id": "_VuTwM4Ji63Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Method                               | Suitable For                                          | Why It’s Good for Inference                                                                                  |\n",
        "| ------------------------------------ | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
        "| **Linear Regression (OLS)**          | Continuous dependent variable                         | Gives direct coefficient estimates → you can interpret how much ( y ) changes with a unit change in ( x_i ). |\n",
        "| **Multiple Linear Regression**       | Continuous ( y ), multiple predictors                 | Helps you see the *partial effect* of each predictor while holding others constant.                          |\n",
        "| **Logistic Regression**              | Binary classification (e.g., success/failure)         | Estimates *odds ratios* — you can interpret how each predictor affects the log-odds of success.              |\n",
        "| **Poisson Regression**               | Count data (e.g., number of sales, clicks)            | Models the relationship between predictors and count outcomes.                                               |\n",
        "| **ANOVA (Analysis of Variance)**     | Categorical predictors, continuous dependent variable | Tests whether group means differ significantly.                                                              |\n",
        "| **ANCOVA (Analysis of Covariance)**  | Continuous + categorical predictors                   | Tests effects of categorical predictors while controlling continuous ones.                                   |\n",
        "| **Generalized Linear Models (GLMs)** | Various data types                                    | Extend linear regression to non-normal distributions (logistic, Poisson, etc.).                              |\n",
        "| **Linear Mixed Models**              | Hierarchical or grouped data                          | Useful when data have random effects (e.g., CEOs nested within companies).                                   |\n",
        "| **Stepwise Regression / LASSO**      | Variable selection                                    | Identifies which predictors matter most (good for inference with many predictors).                           |\n"
      ],
      "metadata": {
        "id": "MB3fQGVckREw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction problem cheatsheet\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0zcpczyBjil4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Method                                    | Type            | Flexibility | Notes                                       |\n",
        "| ----------------------------------------- | --------------- | ----------- | ------------------------------------------- |\n",
        "| **Linear Regression**                     | Inflexible      | Low         | Good for simple, linear trends              |\n",
        "| **Polynomial Regression**                 | Flexible        | Medium      | Captures non-linear patterns                |\n",
        "| **Decision Tree Regressor**               | Flexible        | Medium      | Splits data into regions; interpretable     |\n",
        "| **Random Forest Regressor**               | Very Flexible   | High        | Combines many trees; reduces overfitting    |\n",
        "| **Gradient Boosting (XGBoost, LightGBM)** | Very Flexible   | High        | Excellent predictive accuracy               |\n",
        "| **k-Nearest Neighbors (KNN)**             | Flexible        | Medium      | Based on local neighborhood patterns        |\n",
        "| **Support Vector Regression (SVR)**       | Flexible        | High        | Works well for complex boundaries           |\n",
        "| **Neural Networks / Deep Learning**       | Highly Flexible | Very High   | For complex, high-dimensional relationships |\n"
      ],
      "metadata": {
        "id": "22VQV6UvmVK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Method                                              | Type            | Flexibility | Notes                                             |\n",
        "| --------------------------------------------------- | --------------- | ----------- | ------------------------------------------------- |\n",
        "| **Logistic Regression**                             | Inflexible      | Low         | Good for simple binary classification             |\n",
        "| **Naive Bayes**                                     | Inflexible      | Medium      | Based on probability & Bayes theorem              |\n",
        "| **Decision Tree Classifier**                        | Flexible        | Medium      | Interpretable and visual                          |\n",
        "| **Random Forest Classifier**                        | Flexible        | High        | Reduces overfitting, robust                       |\n",
        "| **Gradient Boosting (XGBoost, CatBoost, LightGBM)** | Very Flexible   | High        | Among the best for structured data                |\n",
        "| **k-Nearest Neighbors (KNN)**                       | Flexible        | Medium      | Simple but sensitive to noise                     |\n",
        "| **Support Vector Machine (SVM)**                    | Flexible        | High        | Works well for complex classification boundaries  |\n",
        "| **Neural Networks / Deep Learning**                 | Highly Flexible | Very High   | For large and complex datasets (e.g. image, text) |\n"
      ],
      "metadata": {
        "id": "QVGCNujfkUL1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p8iOR0SUUKZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}